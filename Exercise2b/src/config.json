{
    "model_params": {
      "input_size": 7056,
      "hidden_layers": [256, 128, 64],
      "output_size": 9,
      "activation": "relu"
    },
    "training_params": {
      "batch_size": 64,
      "lr": 0.0005,
      "epochs": 30
    }
}
